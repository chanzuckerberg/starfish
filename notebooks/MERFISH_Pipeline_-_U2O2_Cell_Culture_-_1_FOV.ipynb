{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce Published results with Starfish\n",
    "\n",
    "This notebook walks through a workflow that reproduces a MERFISH result for one field of view using the starfish package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from showit import image as show_image\n",
    "from starfish.constants import Indices, Features\n",
    "from starfish.experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from cloudfront\n",
    "experiment = Experiment()\n",
    "experiment.read('https://dmf0bdeheu4zf.cloudfront.net/20180802/MERFISH/fov_001/experiment.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual imaging rounds and channels can also be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_image = experiment.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all imaging rounds of channel 0\n",
    "primary_image.show_stack({Indices.CH: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show input file format that specifies how the tiff stack is organized\n",
    "\n",
    "The stack contains multiple images corresponding to the channel and imaging rounds. MERFISH builds a 16 bit barcode from 8 imaging rounds, each of which measures two channels that correspond to contiguous (but not necessarily consistently ordered) bits of the barcode. \n",
    "\n",
    "The MERFISH computational pipeline also constructs a scalar that corrects for intensity differences across each of the 16 images, e.g., one scale factor per bit position.\n",
    "\n",
    "The stacks in this example are pre-registered using fiduciary beads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(experiment.format_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MERFISH codebook maps each barcode to a gene (or blank) feature. The codes in the MERFISH codebook are constructed from a 4-hamming error correcting code with exactly 4 \"on\" bits per barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish import Codebook\n",
    "codebook = Codebook.from_json('https://dmf0bdeheu4zf.cloudfront.net/20180802/MERFISH/codebook.json')\n",
    "codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and scale raw data before decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin filtering with a high pass filter to remove background signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.image import Filter\n",
    "ghp = Filter.GaussianHighPass(sigma=3)\n",
    "ghp.run(primary_image, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below algorithm deconvolves out the point spread function introduced by the microcope and is specifically designed for this use case. The number of iterations is an important parameter that needs careful optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpsf = Filter.DeconvolvePSF(num_iter=15, sigma=2)\n",
    "dpsf.run(primary_image, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the image is pre-registered, as stated above. Despite this, individual RNA molecules may still not be perfectly aligned across imaging rounds. This is crucial in order to read out a measure of the itended barcode (across imaging rounds) in order to map it to the codebook. To solve for potential mis-alignment, the images can be blurred with a 1-pixel Gaussian kernel. The risk here is that this will obfuscate signals from nearby molecules. \n",
    "\n",
    "A local search in pixel space across imaging rounds can also solve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glp = Filter.GaussianLowPass(sigma=1, verbose=True)\n",
    "glp.run(primary_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use MERFISH-calculated size factors to scale the channels across the imaging rounds and visualize the resulting filtered and scaled images. Right now we have to extract this information from the metadata and apply this transformation manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert a small portion of the tensor remains unchanged to check for modifications to the filters\n",
    "expected = np.array(\n",
    "    [[14, 18, 15,  9,  4,  2,  2,  3,  6, 14],\n",
    "     [22, 25, 18,  8,  2,  0,  0,  1,  2,  6],\n",
    "     [24, 26, 16,  6,  1,  0,  0,  0,  0,  2],\n",
    "     [18, 18, 10,  3,  0,  0,  0,  0,  0,  1],\n",
    "     [ 9,  8,  4,  1,  0,  0,  0,  0,  2,  3],\n",
    "     [ 5,  4,  2,  0,  0,  0,  1,  4,  8, 11],\n",
    "     [ 8,  6,  3,  1,  0,  1,  4, 11, 20, 26],\n",
    "     [20, 17, 10,  4,  2,  2,  6, 16, 27, 34],\n",
    "     [37, 33, 20,  9,  4,  4,  7, 13, 21, 25],\n",
    "     [48, 44, 27, 13,  6,  5,  5,  7, 10, 11]], \n",
    ")\n",
    "assert np.array_equal(primary_image.numpy_array[5, 1, 0, 1000:1010, 1000:1010], expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors = {\n",
    "    (t[Indices.ROUND], t[Indices.CH]): t['scale_factor'] \n",
    "    for index, t in primary_image.tile_metadata.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's important to convert the data to float here to retain the correct precision for the scaling. Later, starfish\n",
    "# will operate entirely on float data and this cast can be removed\n",
    "primary_image._data = primary_image._data.astype(np.float64)\n",
    "\n",
    "# this is a scaling method. It would be great to use image.apply here. It's possible, but we need to expose H & C to \n",
    "# at least we can do it with get_slice and set_slice right now.\n",
    "\n",
    "for indices in primary_image._iter_indices():\n",
    "    data = primary_image.get_slice(indices)[0]\n",
    "    scaled = data / scale_factors[indices[Indices.ROUND.value], indices[Indices.CH.value]]\n",
    "    primary_image.set_slice(indices, scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert on a small part of the scaled image to verify values\n",
    "expected = np.array(\n",
    "    [[0.33527313, 0.43106545, 0.35922121, 0.21553273, 0.09579232,\n",
    "      0.04789616, 0.04789616, 0.07184424, 0.14368848, 0.33527313],\n",
    "     [0.52685777, 0.59870201, 0.43106545, 0.19158464, 0.04789616,\n",
    "      0.        , 0.        , 0.02394808, 0.04789616, 0.14368848],\n",
    "     [0.57475393, 0.62265009, 0.38316929, 0.14368848, 0.02394808,\n",
    "      0.        , 0.        , 0.        , 0.        , 0.04789616],\n",
    "     [0.43106545, 0.43106545, 0.23948081, 0.07184424, 0.        ,\n",
    "      0.        , 0.        , 0.        , 0.        , 0.02394808],\n",
    "     [0.21553273, 0.19158464, 0.09579232, 0.02394808, 0.        ,\n",
    "      0.        , 0.        , 0.        , 0.04789616, 0.07184424]]\n",
    ")\n",
    "assert np.allclose(primary_image.numpy_array[5, 1, 0, 1000:1005, 1000:1010], expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import scoreatpercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = primary_image.max_proj(Indices.ROUND, Indices.CH, Indices.Z)\n",
    "clim = scoreatpercentile(mp, [0.5, 99.5])\n",
    "show_image(mp, clim=clim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spot-detector to create 'encoder' table  for standardized input  to decoder\n",
    "\n",
    "Each pipeline exposes a spot detector, and this spot detector translates the filtered image into an encoded table by detecting spots. The table contains the spot_id, the corresponding intensity (v) and the channel (c), imaging round (r) of each spot. \n",
    "\n",
    "The MERFISH pipeline merges these two steps together by finding pixel-based features, and then later collapsing these into spots and filtering out undesirable (non-spot) features. \n",
    "\n",
    "Therefore, no encoder table is generated, but a robust SpotAttribute and DecodedTable are both produced:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode\n",
    "\n",
    "Each assay type also exposes a decoder. A decoder translates each spot (spot_id) in the encoded table into a gene that matches a barcode in the codebook. The goal is to decode and output a quality score, per spot, that describes the confidence in the decoding. Recall that in the MERFISH pipeline, each 'spot' is actually a 16 dimensional vector, one per pixel in the image. From here on, we will refer to these as pixel vectors. Once these pixel vectors are decoded into gene values, contiguous pixels that are decoded to the same gene are labeled as 'spots' via a connected components labeler. We shall refer to the latter as spots.\n",
    "\n",
    "There are hard and soft decodings -- hard decoding is just looking for the max value in the code book. Soft decoding, by contrast, finds the closest code by distance in intensity. Because different assays each have their own intensities and error modes, we leave decoders as user-defined functions. \n",
    "\n",
    "For MERFISH, which uses soft decoding, there are several parameters which are important to determining the result of the decoding method: \n",
    "\n",
    "### Distance threshold\n",
    "In MERFISH, each pixel vector is a 16d vector that we want to map onto a barcode via minimum euclidean distance. Each barcode in the codebook, and each pixel vector is first mapped to the unit sphere by L2 normalization. As such, the maximum distance between a pixel vector and the nearest single-bit error barcode is 0.5176. As such, the decoder only accepts pixel vectors that are below this distance for assignment to a codeword in the codebook. \n",
    "\n",
    "### Magnitude threshold\n",
    "This is a signal floor for decoding. Pixel vectors with an L2 norm below this floor are not considered for decoding. \n",
    "\n",
    "### Area threshold\n",
    "Contiguous pixels that decode to the same gene are called as spots via connected components labeling. The minimum area of these spots are set by this parameter. The intuition is that pixel vectors, that pass the distance and magnitude thresholds, shold probably not be trusted as genes as the mRNA transcript would be too small for them to be real. This parameter can be set based on microscope resolution and signal amplification strategy.\n",
    "\n",
    "### Crop size \n",
    "The crop size crops the image by a number of pixels large enough to eliminate parts of the image that suffer from boundary effects from both signal aquisition (e.g., FOV overlap) and image processing. Here this value is 40.\n",
    "\n",
    "Given these three thresholds, for each pixel vector, the decoder picks the closest code (minimum distance) that satisfies each of the above thresholds, where the distance is calculated between the code and a normalized intensity vector and throws away subsequent spots that are too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.spots import SpotFinder\n",
    "psd = SpotFinder.PixelSpotDetector(\n",
    "    codebook=codebook,\n",
    "    metric='euclidean',\n",
    "    distance_threshold=0.5176,\n",
    "    magnitude_threshold=1,\n",
    "    min_area=2,\n",
    "    max_area=np.inf,\n",
    "    norm_order=2, \n",
    "    crop_size=(0, 40, 40)\n",
    ")\n",
    "\n",
    "spot_intensities, prop_results = psd.find(primary_image)\n",
    "spot_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify detected number of spots\n",
    "assert spot_intensities.shape == (37742, 2, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to results from paper \n",
    "\n",
    "The below plot aggregates gene copy number across single cells in the field of view and compares the results to the published intensities in the MERFISH paper. \n",
    "\n",
    "To make this match perfectly, run deconvolution 15 times instead of 14. As presented below, STARFISH displays a lower detection rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.read_csv('https://dmf0bdeheu4zf.cloudfront.net/MERFISH/benchmark_results.csv', \n",
    "                    dtype = {'barcode':object})\n",
    "\n",
    "benchmark_counts = bench.groupby('gene')['gene'].count()\n",
    "genes, counts = np.unique(spot_intensities[Features.AXIS][Features.TARGET], return_counts=True)\n",
    "result_counts = pd.Series(counts, index=genes)\n",
    "\n",
    "tmp = pd.concat([result_counts, benchmark_counts], join='inner', axis=1).values\n",
    "\n",
    "r = np.corrcoef(tmp[:, 1], tmp[:, 0])[0, 1]\n",
    "x = np.linspace(50, 2000)\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(tmp[:, 1], tmp[:, 0], 50, zorder=2)\n",
    "ax.plot(x, x, '-k', zorder=1)\n",
    "\n",
    "plt.xlabel('Gene copy number Benchmark')\n",
    "plt.ylabel('Gene copy number Starfish')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title(f'r = {r}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that notebook results match published ones. \n",
    "assert r > 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "This image applies a pseudo-color to each gene channel to visualize the position and size of all called spots in a subset of the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_lookup = lambda x: 0 if x == 0 else prop_results.region_properties[x - 1].area\n",
    "vfunc = np.vectorize(area_lookup)\n",
    "mask = np.squeeze(vfunc(prop_results.label_image))\n",
    "show_image(np.squeeze(prop_results.decoded_image)*(mask > 2), cmap = 'nipy_spectral')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}